{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 16880,
          "databundleVersionId": 858837,
          "sourceType": "competition"
        },
        {
          "sourceId": 888125,
          "sourceType": "datasetVersion",
          "datasetId": 458848
        },
        {
          "sourceId": 904207,
          "sourceType": "datasetVersion",
          "datasetId": 484608
        },
        {
          "sourceId": 955742,
          "sourceType": "datasetVersion",
          "datasetId": 513681
        },
        {
          "sourceId": 967725,
          "sourceType": "datasetVersion",
          "datasetId": 514243
        },
        {
          "sourceId": 9670710,
          "sourceType": "datasetVersion",
          "datasetId": 5909667
        }
      ],
      "dockerImageVersionId": 29845,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Resnext Model"
      ],
      "metadata": {
        "id": "mgp5Us3rmBc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resnext & Xception Ensemble (Inference)\n",
        "\n",
        "- This kernel outputs the ensemble of the results from https://www.kaggle.com/khoongweihao/frames-per-video-viz and https://www.kaggle.com/greatgamedota/xception-binary-classifier-inference (not original, modified learning rate and epochs)\n",
        "- Frames per video at 64 (best found)"
      ],
      "metadata": {
        "id": "PBydLOqamBc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys, time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-20T00:49:59.296298Z",
          "iopub.execute_input": "2024-10-20T00:49:59.296592Z",
          "iopub.status.idle": "2024-10-20T00:50:01.282141Z",
          "shell.execute_reply.started": "2024-10-20T00:49:59.296547Z",
          "shell.execute_reply": "2024-10-20T00:50:01.281526Z"
        },
        "trusted": true,
        "id": "ZqzexVvLmBc2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_dir = \"/kaggle/input/deepfake-detection-challenge/test_videos/\"\n",
        "\n",
        "test_videos = sorted([x for x in os.listdir(test_dir) if x[-4:] == \".mp4\"])\n",
        "frame_h = 5\n",
        "frame_l = 5\n",
        "len(test_videos)"
      ],
      "metadata": {
        "trusted": true,
        "id": "HKD-pOLNmBc3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA version:\", torch.version.cuda)\n",
        "print(\"cuDNN version:\", torch.backends.cudnn.version())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-20T00:50:11.496139Z",
          "iopub.execute_input": "2024-10-20T00:50:11.496467Z",
          "iopub.status.idle": "2024-10-20T00:50:11.502509Z",
          "shell.execute_reply.started": "2024-10-20T00:50:11.496405Z",
          "shell.execute_reply": "2024-10-20T00:50:11.501740Z"
        },
        "trusted": true,
        "id": "L-AUvuDQmBc3",
        "outputId": "5103ba3c-292c-4ecd-d021-132e2a089ab8"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "PyTorch version: 1.3.0\nCUDA version: 10.0.130\ncuDNN version: 7603\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "gpu"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-20T00:50:14.418193Z",
          "iopub.execute_input": "2024-10-20T00:50:14.418522Z",
          "iopub.status.idle": "2024-10-20T00:50:14.487593Z",
          "shell.execute_reply.started": "2024-10-20T00:50:14.418470Z",
          "shell.execute_reply": "2024-10-20T00:50:14.486688Z"
        },
        "trusted": true,
        "id": "L__Ey5YimBc4",
        "outputId": "5009a22b-36db-431c-d34f-861c7bed680b"
      },
      "outputs": [
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "device(type='cuda', index=0)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"/content/blazeface-pytorch\")\n",
        "sys.path.insert(0, \"/content/deepfakes-inference-demo\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-20T00:50:16.778134Z",
          "iopub.execute_input": "2024-10-20T00:50:16.778448Z",
          "iopub.status.idle": "2024-10-20T00:50:16.782934Z",
          "shell.execute_reply.started": "2024-10-20T00:50:16.778375Z",
          "shell.execute_reply": "2024-10-20T00:50:16.782100Z"
        },
        "trusted": true,
        "id": "9o71AU8XmBc4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from blazeface import BlazeFace\n",
        "facedet = BlazeFace().to(gpu)\n",
        "facedet.load_weights(\"/content/blazeface-pytorch/blazeface.pth\")\n",
        "facedet.load_anchors(\"/content/blazeface-pytorch/anchors.npy\")\n",
        "_ = facedet.train(False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-20T00:50:19.602970Z",
          "iopub.execute_input": "2024-10-20T00:50:19.603264Z",
          "iopub.status.idle": "2024-10-20T00:50:24.152766Z",
          "shell.execute_reply.started": "2024-10-20T00:50:19.603221Z",
          "shell.execute_reply": "2024-10-20T00:50:24.151995Z"
        },
        "trusted": true,
        "id": "ehfZhokCmBc5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from helpers.read_video_1 import VideoReader\n",
        "from helpers.face_extract_1 import FaceExtractor\n",
        "\n",
        "frames_per_video = 64 #frame_h * frame_l\n",
        "video_reader = VideoReader()\n",
        "video_read_fn = lambda x: video_reader.read_frames(x, num_frames=frames_per_video)\n",
        "face_extractor = FaceExtractor(video_read_fn, facedet)"
      ],
      "metadata": {
        "trusted": true,
        "id": "-QM9YbphmBc5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 224"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-20T00:12:24.934960Z",
          "iopub.execute_input": "2024-10-20T00:12:24.935258Z",
          "iopub.status.idle": "2024-10-20T00:12:24.938906Z",
          "shell.execute_reply.started": "2024-10-20T00:12:24.935217Z",
          "shell.execute_reply": "2024-10-20T00:12:24.937987Z"
        },
        "trusted": true,
        "id": "roV7HZWDmBc5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import Normalize\n",
        "\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "normalize_transform = Normalize(mean, std)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-20T00:12:27.274759Z",
          "iopub.execute_input": "2024-10-20T00:12:27.275056Z",
          "iopub.status.idle": "2024-10-20T00:12:27.394740Z",
          "shell.execute_reply.started": "2024-10-20T00:12:27.275014Z",
          "shell.execute_reply": "2024-10-20T00:12:27.394130Z"
        },
        "trusted": true,
        "id": "bgFta4yZmBc5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def isotropically_resize_image(img, size, resample=cv2.INTER_AREA):\n",
        "    h, w = img.shape[:2]\n",
        "    if w > h:\n",
        "        h = h * size // w\n",
        "        w = size\n",
        "    else:\n",
        "        w = w * size // h\n",
        "        h = size\n",
        "\n",
        "    resized = cv2.resize(img, (w, h), interpolation=resample)\n",
        "    return resized\n",
        "\n",
        "\n",
        "def make_square_image(img):\n",
        "    h, w = img.shape[:2]\n",
        "    size = max(h, w)\n",
        "    t = 0\n",
        "    b = size - h\n",
        "    l = 0\n",
        "    r = size - w\n",
        "    return cv2.copyMakeBorder(img, t, b, l, r, cv2.BORDER_CONSTANT, value=0)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-20T00:12:33.124261Z",
          "iopub.execute_input": "2024-10-20T00:12:33.124700Z",
          "iopub.status.idle": "2024-10-20T00:12:33.133504Z",
          "shell.execute_reply.started": "2024-10-20T00:12:33.124505Z",
          "shell.execute_reply": "2024-10-20T00:12:33.132671Z"
        },
        "trusted": true,
        "id": "LsssAnXmmBc5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class MyResNeXt(models.resnet.ResNet):\n",
        "    def __init__(self, training=True):\n",
        "        super(MyResNeXt, self).__init__(block=models.resnet.Bottleneck,\n",
        "                                        layers=[3, 4, 6, 3],\n",
        "                                        groups=32,\n",
        "                                        width_per_group=4)\n",
        "        self.fc = nn.Linear(2048, 1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-20T00:12:39.654334Z",
          "iopub.execute_input": "2024-10-20T00:12:39.654635Z",
          "iopub.status.idle": "2024-10-20T00:12:39.661820Z",
          "shell.execute_reply.started": "2024-10-20T00:12:39.654590Z",
          "shell.execute_reply": "2024-10-20T00:12:39.660603Z"
        },
        "trusted": true,
        "id": "8YuX1TyymBc6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_on_video(video_path, batch_size):\n",
        "    try:\n",
        "        # Find the faces for N frames in the video.\n",
        "        faces = face_extractor.process_video(video_path)\n",
        "\n",
        "        # Only look at one face per frame.\n",
        "        face_extractor.keep_only_best_face(faces)\n",
        "\n",
        "        if len(faces) > 0:\n",
        "            # NOTE: When running on the CPU, the batch size must be fixed\n",
        "            # or else memory usage will blow up. (Bug in PyTorch?)\n",
        "            x = np.zeros((batch_size, input_size, input_size, 3), dtype=np.uint8)\n",
        "\n",
        "            # If we found any faces, prepare them for the model.\n",
        "            n = 0\n",
        "            for frame_data in faces:\n",
        "                for face in frame_data[\"faces\"]:\n",
        "                    # Resize to the model's required input size.\n",
        "                    # We keep the aspect ratio intact and add zero\n",
        "                    # padding if necessary.\n",
        "                    resized_face = isotropically_resize_image(face, input_size)\n",
        "                    resized_face = make_square_image(resized_face)\n",
        "\n",
        "                    if n < batch_size:\n",
        "                        x[n] = resized_face\n",
        "                        n += 1\n",
        "                    else:\n",
        "                        print(\"WARNING: have %d faces but batch size is %d\" % (n, batch_size))\n",
        "\n",
        "                    # Test time augmentation: horizontal flips.\n",
        "                    # TODO: not sure yet if this helps or not\n",
        "                    #x[n] = cv2.flip(resized_face, 1)\n",
        "                    #n += 1\n",
        "\n",
        "            if n > 0:\n",
        "                x = torch.tensor(x, device=gpu).float()\n",
        "\n",
        "                # Preprocess the images.\n",
        "                x = x.permute((0, 3, 1, 2))\n",
        "\n",
        "                for i in range(len(x)):\n",
        "                    x[i] = normalize_transform(x[i] / 255.)\n",
        "\n",
        "                # Make a prediction, then take the average.\n",
        "                with torch.no_grad():\n",
        "                    y_pred = model(x)\n",
        "                    y_pred = torch.sigmoid(y_pred.squeeze())\n",
        "                    return y_pred[:n].mean().item()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Prediction error on video %s: %s\" % (video_path, str(e)))\n",
        "\n",
        "    return 0.5"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-20T00:12:49.201143Z",
          "iopub.execute_input": "2024-10-20T00:12:49.201533Z",
          "iopub.status.idle": "2024-10-20T00:12:49.215092Z",
          "shell.execute_reply.started": "2024-10-20T00:12:49.201458Z",
          "shell.execute_reply": "2024-10-20T00:12:49.214050Z"
        },
        "trusted": true,
        "id": "DKesWkhHmBc6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(\"/content/deepfakes-inference-demo/resnext.pth\", map_location=gpu)\n",
        "\n",
        "model = MyResNeXt().to(gpu)\n",
        "model.load_state_dict(checkpoint)\n",
        "_ = model.eval()\n",
        "\n",
        "del checkpoint"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-20T00:12:44.102743Z",
          "iopub.execute_input": "2024-10-20T00:12:44.103033Z",
          "iopub.status.idle": "2024-10-20T00:12:45.271587Z",
          "shell.execute_reply.started": "2024-10-20T00:12:44.102991Z",
          "shell.execute_reply": "2024-10-20T00:12:45.270840Z"
        },
        "trusted": true,
        "id": "zkQvtS6RmBc6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def predict_on_video_set(videos, num_workers):\n",
        "    def process_file(i):\n",
        "        filename = videos[i]\n",
        "        y_pred = predict_on_video(os.path.join(test_dir, filename), batch_size=frames_per_video)\n",
        "        return y_pred\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as ex:\n",
        "        predictions = ex.map(process_file, range(len(videos)))\n",
        "\n",
        "    return list(predictions)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-20T00:12:54.767212Z",
          "iopub.execute_input": "2024-10-20T00:12:54.767545Z",
          "iopub.status.idle": "2024-10-20T00:12:54.773841Z",
          "shell.execute_reply.started": "2024-10-20T00:12:54.767485Z",
          "shell.execute_reply": "2024-10-20T00:12:54.773143Z"
        },
        "trusted": true,
        "id": "YHWp84K1mBc6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "def process_video_frames(video_path, max_frames=64):\n",
        "    \"\"\"\n",
        "    Reads a video file, extracts frames, and processes them into tensors.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): Path to the video file.\n",
        "        max_frames (int): Maximum number of frames to extract from the video.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Tensor containing the processed frames.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((299, 299)),  # Resizing frames for Xception\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    frame_count = 0\n",
        "    while cap.isOpened() and frame_count < max_frames:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "        frame_tensor = transform(frame)  # Apply transformations\n",
        "        frames.append(frame_tensor)\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "#Stack frames into a tensor (assuming max_frames)\n",
        "    return torch.stack(frames)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-20T00:18:22.615602Z",
          "iopub.execute_input": "2024-10-20T00:18:22.615901Z",
          "iopub.status.idle": "2024-10-20T00:18:22.625805Z",
          "shell.execute_reply.started": "2024-10-20T00:18:22.615859Z",
          "shell.execute_reply": "2024-10-20T00:18:22.624816Z"
        },
        "id": "7q-PCtbWmBc6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "from blazeface import BlazeFace\n",
        "#from helpers.read_video_1 import process_video_frames  # Ensure this script is uploaded\n",
        "\n",
        "# Define UADFV Directory\n",
        "UADFV_dir = \"/kaggle/input/videos\"\n",
        "\n",
        "# Dataset class for UADFV videos\n",
        "class UADFVVideoDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.video_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Load video paths and their corresponding labels\n",
        "        for label in ['real', 'fake']:\n",
        "            label_dir = os.path.join(data_dir, label)\n",
        "            for video_file in os.listdir(label_dir):\n",
        "                self.video_paths.append(os.path.join(label_dir, video_file))\n",
        "                self.labels.append(0 if label == 'real' else 1)  # Assign 0 for real, 1 for fake\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.video_paths[idx]\n",
        "        video_frames = process_video_frames(video_path)  # Extract frames from video\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            video_frames = self.transform(video_frames)\n",
        "\n",
        "        return video_frames, label\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-20T00:18:38.682560Z",
          "iopub.execute_input": "2024-10-20T00:18:38.682864Z",
          "iopub.status.idle": "2024-10-20T00:18:38.694763Z",
          "shell.execute_reply.started": "2024-10-20T00:18:38.682815Z",
          "shell.execute_reply": "2024-10-20T00:18:38.694015Z"
        },
        "id": "DfPRDipDmBc6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ../input/deepfake-xception-trained-model/pytorchcv-0.0.55-py2.py3-none-any.whl --quiet"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-20T00:20:44.446447Z",
          "iopub.execute_input": "2024-10-20T00:20:44.446743Z",
          "iopub.status.idle": "2024-10-20T00:21:12.330484Z",
          "shell.execute_reply.started": "2024-10-20T00:20:44.446700Z",
          "shell.execute_reply": "2024-10-20T00:21:12.329568Z"
        },
        "id": "6kCPCp04mBc7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "Scale pixel values from [0, 255] to [0, 1]\n",
        "def scale_and_normalize(image_tensor):\n",
        "    if len(image_tensor.shape) == 3:  # Expecting [C, H, W]\n",
        "        image_tensor = image_tensor / 255.0  # Scale to [0, 1]\n",
        "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        return normalize(image_tensor)\n",
        "    else:\n",
        "        raise TypeError(f\"Expected tensor of shape [C, H, W], but got {image_tensor.shape}\")\n",
        "\n",
        "#Apply directly to video frames\n",
        "#"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-20T00:27:21.339945Z",
          "iopub.execute_input": "2024-10-20T00:27:21.340278Z",
          "iopub.status.idle": "2024-10-20T00:27:21.346149Z",
          "shell.execute_reply.started": "2024-10-20T00:27:21.340229Z",
          "shell.execute_reply": "2024-10-20T00:27:21.345212Z"
        },
        "id": "D0wlP2B7mBc7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Data transformations (if needed)\n",
        "\"\"\"transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\"\"\"\n",
        "\n",
        "\n",
        "# Load BlazeFace Model for face detection\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "facedet = BlazeFace().to(device)\n",
        "facedet.load_weights('/kaggle/input/blazeface-pytorch/blazeface.pth')  # Load BlazeFace weights\n",
        "facedet.load_anchors('/kaggle/input/blazeface-pytorch/anchors.npy')    # Load BlazeFace anchors\n",
        "facedet.train(False)\n",
        "\n",
        "# Load Xception model with your custom implementation\n",
        "from pytorchcv.model_provider import get_model\n",
        "class FCN(nn.Module):\n",
        "    def __init__(self, base, in_f):\n",
        "        super(FCN, self).__init__()\n",
        "        self.base = base\n",
        "        self.h1 = Head(in_f, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base(x)\n",
        "        return self.h1(x)\n",
        "\n",
        "# Load Xception model and remove the output layer\n",
        "xception_base = get_model(\"xception\", pretrained=False)\n",
        "xception_base = nn.Sequential(*list(xception_base.children())[:-1])\n",
        "\n",
        "# Replace final block's pooling layer\n",
        "xception_base[0].final_block.pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)))\n",
        "\n",
        "# Create the full Xception model with a custom head\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, in_f, out_f):\n",
        "        super(Head, self).__init__()\n",
        "        self.f = nn.Flatten()\n",
        "        self.l = nn.Linear(in_f, 512)\n",
        "        self.d = nn.Dropout(0.5)\n",
        "        self.o = nn.Linear(512, out_f)\n",
        "        self.b1 = nn.BatchNorm1d(in_f)\n",
        "        self.b2 = nn.BatchNorm1d(512)\n",
        "        self.r = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.f(x)\n",
        "        x = self.b1(x)\n",
        "        x = self.d(x)\n",
        "        x = self.l(x)\n",
        "        x = self.r(x)\n",
        "        x = self.b2(x)\n",
        "        x = self.d(x)\n",
        "        return self.o(x)\n",
        "\n",
        "xception_model = FCN(xception_base, 2048)\n",
        "xception_model = xception_model.to(device)\n",
        "xception_model.load_state_dict(torch.load('../input/deepfake-xception-trained-model/model.pth'))\n",
        "\n",
        "# Load ResNeXt model with your custom implementation\n",
        "import torchvision.models as models\n",
        "class MyResNeXt(models.resnet.ResNet):\n",
        "    def __init__(self, training=True):\n",
        "        super(MyResNeXt, self).__init__(block=models.resnet.Bottleneck,\n",
        "                                        layers=[3, 4, 6, 3],\n",
        "                                        groups=32,\n",
        "                                        width_per_group=4)\n",
        "        self.fc = nn.Linear(2048, 1)\n",
        "\n",
        "resnext_model = MyResNeXt().to(device)\n",
        "resnext_model.load_state_dict(torch.load('/kaggle/input/deepfakes-inference-demo/resnext.pth'))\n",
        "\n",
        "# Load dataset\n",
        "test_dataset = UADFVVideoDataset(data_dir=UADFV_dir + '/test', transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Inference on test data\n",
        "def predict_on_test_data():\n",
        "    xception_model.eval()\n",
        "    resnext_model.eval()\n",
        "\n",
        "    for videos, labels in test_loader:\n",
        "        videos = videos.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            xception_output = xception_model(videos)\n",
        "            resnext_output = resnext_model(videos)\n",
        "\n",
        "            # Average the predictions from both models (ensemble)\n",
        "            final_output = (xception_output + resnext_output) / 2\n",
        "            prediction = torch.argmax(final_output, dim=1)\n",
        "\n",
        "            print(f\"Prediction: {'Deepfake' if prediction.item() == 1 else 'Real'}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-20T00:27:32.113235Z",
          "iopub.execute_input": "2024-10-20T00:27:32.113517Z",
          "iopub.status.idle": "2024-10-20T00:27:32.956694Z",
          "shell.execute_reply.started": "2024-10-20T00:27:32.113482Z",
          "shell.execute_reply": "2024-10-20T00:27:32.955872Z"
        },
        "id": "RM7EUcTlmBc7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XOC7tQ6pmBc7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning on the training data\n",
        "train_dataset = UADFVVideoDataset(data_dir=UADFV_dir + '/train', transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Fine-tuning setup\n",
        "for param in xception_model.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in resnext_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze final layers for training\n",
        "for param in xception_model.h1.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in resnext_model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer_xception = optim.Adam(xception_model.h1.parameters(), lr=0.0001)\n",
        "optimizer_resnext = optim.Adam(resnext_model.fc.parameters(), lr=0.0001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    xception_model.train()\n",
        "    resnext_model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for videos, labels in train_loader:\n",
        "        videos, labels = videos.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer_xception.zero_grad()\n",
        "        optimizer_resnext.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        xception_output = xception_model(videos)\n",
        "        resnext_output = resnext_model(videos)\n",
        "\n",
        "        # Average predictions\n",
        "        final_output = (xception_output + resnext_output) / 2\n",
        "        loss = criterion(final_output, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer_xception.step()\n",
        "        optimizer_resnext.step()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        running_loss += loss.item()\n",
        "        predicted = (torch.sigmoid(final_output) > 0.5).float()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# Save the fine-tuned models\n",
        "torch.save(xception_model.state_dict(), 'fine_tuned_xception.pth')\n",
        "torch.save(resnext_model.state_dict(), 'fine_tuned_resnext.pth')\n",
        "\n",
        "# Run predictions on test data\n",
        "predict_on_test_data()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-20T00:27:41.040017Z",
          "iopub.execute_input": "2024-10-20T00:27:41.040355Z",
          "iopub.status.idle": "2024-10-20T00:27:41.459583Z",
          "shell.execute_reply.started": "2024-10-20T00:27:41.040307Z",
          "shell.execute_reply": "2024-10-20T00:27:41.458358Z"
        },
        "id": "e1WETw6_mBc7",
        "outputId": "0c72d558-c796-43d0-88d9-137afe1c3bce"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-60de9d2dca9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mvideos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mvideos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-11932b15d821>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mvideo_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvideo_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \"\"\"\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_tensor_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tensor is not a torch image.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: tensor is not a torch image."
          ],
          "ename": "TypeError",
          "evalue": "tensor is not a torch image.",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "d_6SamAkmBc7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kamalna ---------------------\n"
      ],
      "metadata": {
        "id": "DpYE49CjmBc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "speed_test = False  # you have to enable this manually"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-19T22:54:28.268903Z",
          "iopub.execute_input": "2024-10-19T22:54:28.269230Z",
          "iopub.status.idle": "2024-10-19T22:54:28.273195Z",
          "shell.execute_reply.started": "2024-10-19T22:54:28.269172Z",
          "shell.execute_reply": "2024-10-19T22:54:28.272209Z"
        },
        "trusted": true,
        "id": "D1R7GjRwmBc7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "if speed_test:\n",
        "    start_time = time.time()\n",
        "    speedtest_videos = test_videos[:5]\n",
        "    predictions = predict_on_video_set(speedtest_videos, num_workers=4)\n",
        "    elapsed = time.time() - start_time\n",
        "    print(\"Elapsed %f sec. Average per video: %f sec.\" % (elapsed, elapsed / len(speedtest_videos)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-19T22:54:28.676007Z",
          "iopub.execute_input": "2024-10-19T22:54:28.676275Z",
          "iopub.status.idle": "2024-10-19T22:54:43.548329Z",
          "shell.execute_reply.started": "2024-10-19T22:54:28.676233Z",
          "shell.execute_reply": "2024-10-19T22:54:43.547477Z"
        },
        "trusted": true,
        "id": "-H1JlDpkmBc8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = predict_on_video_set(test_videos[:5], num_workers=4)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-19T23:04:57.494094Z",
          "iopub.execute_input": "2024-10-19T23:04:57.494500Z",
          "iopub.status.idle": "2024-10-19T23:05:27.322783Z",
          "shell.execute_reply.started": "2024-10-19T23:04:57.494445Z",
          "shell.execute_reply": "2024-10-19T23:05:27.321595Z"
        },
        "trusted": true,
        "id": "N-30PYe2mBc8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df_resnext = pd.DataFrame({\"filename\": test_videos[:5], \"label\": predictions})\n",
        "submission_df_resnext.to_csv(\"submission_resnext.csv\", index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-19T23:05:53.748999Z",
          "iopub.execute_input": "2024-10-19T23:05:53.749421Z",
          "iopub.status.idle": "2024-10-19T23:05:54.836961Z",
          "shell.execute_reply.started": "2024-10-19T23:05:53.749334Z",
          "shell.execute_reply": "2024-10-19T23:05:54.835849Z"
        },
        "trusted": true,
        "id": "LAzIC84PmBc8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Xception Net"
      ],
      "metadata": {
        "id": "yxg9X8KsmBc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ../input/deepfake-xception-trained-model/pytorchcv-0.0.55-py2.py3-none-any.whl --quiet"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-19T23:14:04.073310Z",
          "iopub.execute_input": "2024-10-19T23:14:04.073699Z",
          "iopub.status.idle": "2024-10-19T23:14:32.003713Z",
          "shell.execute_reply.started": "2024-10-19T23:14:04.073629Z",
          "shell.execute_reply": "2024-10-19T23:14:32.002657Z"
        },
        "trusted": true,
        "id": "U_fo1XDYmBc8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_dir = \"/kaggle/input/deepfake-detection-challenge/test_videos/\"\n",
        "\n",
        "test_videos = sorted([x for x in os.listdir(test_dir) if x[-4:] == \".mp4\"])\n",
        "len(test_videos)"
      ],
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "cpb-AYakmBc8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "967VtZormBc8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"/kaggle/input/blazeface-pytorch\")\n",
        "sys.path.insert(0, \"/kaggle/input/deepfakes-inference-demo\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "FK212FP2mBc8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from blazeface import BlazeFace\n",
        "facedet = BlazeFace().to(gpu)\n",
        "facedet.load_weights(\"/kaggle/input/blazeface-pytorch/blazeface.pth\")\n",
        "facedet.load_anchors(\"/kaggle/input/blazeface-pytorch/anchors.npy\")\n",
        "_ = facedet.train(False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "WgME4YF4mBc8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from helpers.read_video_1 import VideoReader\n",
        "from helpers.face_extract_1 import FaceExtractor\n",
        "\n",
        "frames_per_video = 64 # originally 4\n",
        "\n",
        "video_reader = VideoReader()\n",
        "video_read_fn = lambda x: video_reader.read_frames(x, num_frames=frames_per_video)\n",
        "face_extractor = FaceExtractor(video_read_fn, facedet)"
      ],
      "metadata": {
        "trusted": true,
        "id": "2C9rV020mBc9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 150"
      ],
      "metadata": {
        "trusted": true,
        "id": "vyFgIzODmBc9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import Normalize\n",
        "\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "normalize_transform = Normalize(mean, std)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Z0Z7kJSImBc9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def isotropically_resize_image(img, size, resample=cv2.INTER_AREA):\n",
        "    h, w = img.shape[:2]\n",
        "    if w > h:\n",
        "        h = h * size // w\n",
        "        w = size\n",
        "    else:\n",
        "        w = w * size // h\n",
        "        h = size\n",
        "\n",
        "    resized = cv2.resize(img, (w, h), interpolation=resample)\n",
        "    return resized\n",
        "\n",
        "\n",
        "def make_square_image(img):\n",
        "    h, w = img.shape[:2]\n",
        "    size = max(h, w)\n",
        "    t = 0\n",
        "    b = size - h\n",
        "    l = 0\n",
        "    r = size - w\n",
        "    return cv2.copyMakeBorder(img, t, b, l, r, cv2.BORDER_CONSTANT, value=0)"
      ],
      "metadata": {
        "trusted": true,
        "id": "2R84gsCDmBc9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ../input/deepfake-xception-trained-model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-19T23:42:25.362100Z",
          "iopub.execute_input": "2024-10-19T23:42:25.362419Z",
          "iopub.status.idle": "2024-10-19T23:42:26.379347Z",
          "shell.execute_reply.started": "2024-10-19T23:42:25.362377Z",
          "shell.execute_reply": "2024-10-19T23:42:26.378566Z"
        },
        "trusted": true,
        "id": "K_ESec89mBc9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5n1IpCYwmBc9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Xception model with your custom implementation\n",
        "from pytorchcv.model_provider import get_model\n",
        "class FCN(nn.Module):\n",
        "    def __init__(self, base, in_f):\n",
        "        super(FCN, self).__init__()\n",
        "        self.base = base\n",
        "        self.h1 = Head(in_f, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base(x)\n",
        "        return self.h1(x)\n",
        "\n",
        "# Load Xception model and remove the output layer\n",
        "xception_base = get_model(\"xception\", pretrained=False)\n",
        "xception_base = nn.Sequential(*list(xception_base.children())[:-1])\n",
        "\n",
        "# Replace final block's pooling layer\n",
        "xception_base[0].final_block.pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)))\n",
        "\n",
        "# Create the full Xception model with a custom head\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, in_f, out_f):\n",
        "        super(Head, self).__init__()\n",
        "        self.f = nn.Flatten()\n",
        "        self.l = nn.Linear(in_f, 512)\n",
        "        self.d = nn.Dropout(0.5)\n",
        "        self.o = nn.Linear(512, out_f)\n",
        "        self.b1 = nn.BatchNorm1d(in_f)\n",
        "        self.b2 = nn.BatchNorm1d(512)\n",
        "        self.r = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.f(x)\n",
        "        x = self.b1(x)\n",
        "        x = self.d(x)\n",
        "        x = self.l(x)\n",
        "        x = self.r(x)\n",
        "        x = self.b2(x)\n",
        "        x = self.d(x)\n",
        "        return self.o(x)\n",
        "\n",
        "xception_model = FCN(xception_base, 2048)\n",
        "xception_model = xception_model.to(device)\n",
        "xception_model.load_state_dict(torch.load('../input/deepfake-xception-trained-model/model.pth'))\n"
      ],
      "metadata": {
        "id": "TyLVn8admBc9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorchcv.model_provider import get_model\n",
        "model = get_model(\"xception\", pretrained=False)\n",
        "model = nn.Sequential(*list(model.children())[:-1]) # Remove original output layer\n",
        "\n",
        "class Pooling(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Pooling, self).__init__()\n",
        "\n",
        "    self.p1 = nn.AdaptiveAvgPool2d((1,1))\n",
        "    self.p2 = nn.AdaptiveMaxPool2d((1,1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x1 = self.p1(x)\n",
        "    x2 = self.p2(x)\n",
        "    return (x1+x2) * 0.5\n",
        "\n",
        "model[0].final_block.pool = nn.Sequential(nn.AdaptiveAvgPool2d((1,1)))\n",
        "\n",
        "class Head(torch.nn.Module):\n",
        "  def __init__(self, in_f, out_f):\n",
        "    super(Head, self).__init__()\n",
        "\n",
        "    self.f = nn.Flatten()\n",
        "    self.l = nn.Linear(in_f, 512)\n",
        "    self.d = nn.Dropout(0.5)\n",
        "    self.o = nn.Linear(512, out_f)\n",
        "    self.b1 = nn.BatchNorm1d(in_f)\n",
        "    self.b2 = nn.BatchNorm1d(512)\n",
        "    self.r = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.f(x)\n",
        "    x = self.b1(x)\n",
        "    x = self.d(x)\n",
        "\n",
        "    x = self.l(x)\n",
        "    x = self.r(x)\n",
        "    x = self.b2(x)\n",
        "    x = self.d(x)\n",
        "\n",
        "    out = self.o(x)\n",
        "    return out\n",
        "\n",
        "class FCN(torch.nn.Module):\n",
        "  def __init__(self, base, in_f):\n",
        "    super(FCN, self).__init__()\n",
        "    self.base = base\n",
        "    self.h1 = Head(in_f, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.base(x)\n",
        "    return self.h1(x)\n",
        "\n",
        "net = []\n",
        "model = FCN(model, 2048)\n",
        "model = model.cuda()\n",
        "model.load_state_dict(torch.load('../input/deepfake-xception-trained-model/model.pth')) # new, updated\n",
        "net.append(model)"
      ],
      "metadata": {
        "trusted": true,
        "id": "NWus0BdGmBc-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction loop"
      ],
      "metadata": {
        "id": "LpNkTy4vmBc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_on_video(video_path, batch_size):\n",
        "    try:\n",
        "        # Find the faces for N frames in the video.\n",
        "        faces = face_extractor.process_video(video_path)\n",
        "\n",
        "        # Only look at one face per frame.\n",
        "        face_extractor.keep_only_best_face(faces)\n",
        "\n",
        "        if len(faces) > 0:\n",
        "            # NOTE: When running on the CPU, the batch size must be fixed\n",
        "            # or else memory usage will blow up. (Bug in PyTorch?)\n",
        "            x = np.zeros((batch_size, input_size, input_size, 3), dtype=np.uint8)\n",
        "\n",
        "            # If we found any faces, prepare them for the model.\n",
        "            n = 0\n",
        "            for frame_data in faces:\n",
        "                for face in frame_data[\"faces\"]:\n",
        "                    # Resize to the model's required input size.\n",
        "                    # We keep the aspect ratio intact and add zero\n",
        "                    # padding if necessary.\n",
        "                    resized_face = isotropically_resize_image(face, input_size)\n",
        "                    resized_face = make_square_image(resized_face)\n",
        "\n",
        "                    if n < batch_size:\n",
        "                        x[n] = resized_face\n",
        "                        n += 1\n",
        "                    else:\n",
        "                        print(\"WARNING: have %d faces but batch size is %d\" % (n, batch_size))\n",
        "\n",
        "                    # Test time augmentation: horizontal flips.\n",
        "                    # TODO: not sure yet if this helps or not\n",
        "                    #x[n] = cv2.flip(resized_face, 1)\n",
        "                    #n += 1\n",
        "\n",
        "            if n > 0:\n",
        "                x = torch.tensor(x, device=gpu).float()\n",
        "\n",
        "                # Preprocess the images.\n",
        "                x = x.permute((0, 3, 1, 2))\n",
        "\n",
        "                for i in range(len(x)):\n",
        "                    x[i] = normalize_transform(x[i] / 255.)\n",
        "#                     x[i] = x[i] / 255.\n",
        "\n",
        "                # Make a prediction, then take the average.\n",
        "                with torch.no_grad():\n",
        "                    y_pred = model(x)\n",
        "                    y_pred = torch.sigmoid(y_pred.squeeze())\n",
        "                    return y_pred[:n].mean().item()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Prediction error on video %s: %s\" % (video_path, str(e)))\n",
        "\n",
        "    return 0.5"
      ],
      "metadata": {
        "trusted": true,
        "id": "FnyNLNBumBc-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def predict_on_video_set(videos, num_workers):\n",
        "    def process_file(i):\n",
        "        filename = videos[i]\n",
        "        y_pred = predict_on_video(os.path.join(test_dir, filename), batch_size=frames_per_video)\n",
        "        return y_pred\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as ex:\n",
        "        predictions = ex.map(process_file, range(len(videos)))\n",
        "\n",
        "    return list(predictions)"
      ],
      "metadata": {
        "trusted": true,
        "id": "HrPQfVTBmBc-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "speed_test = False"
      ],
      "metadata": {
        "trusted": true,
        "id": "Mq5C5I26mBc-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "if speed_test:\n",
        "    start_time = time.time()\n",
        "    speedtest_videos = test_videos[:5]\n",
        "    predictions = predict_on_video_set(speedtest_videos, num_workers=4)\n",
        "    elapsed = time.time() - start_time\n",
        "    print(\"Elapsed %f sec. Average per video: %f sec.\" % (elapsed, elapsed / len(speedtest_videos)))"
      ],
      "metadata": {
        "trusted": true,
        "id": "rbe5Na0XmBc-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "model.eval()\n",
        "predictions = predict_on_video_set(test_videos, num_workers=4)"
      ],
      "metadata": {
        "trusted": true,
        "id": "hGxDf_JTmBc-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df_xception = pd.DataFrame({\"filename\": test_videos, \"label\": predictions})\n",
        "submission_df_xception.to_csv(\"submission_xception.csv\", index=False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Xe80UWrtmBc-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df_resnext.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "x96geG3XmBc-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df_xception.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "BUB4C80vmBc-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load ResNeXt model with your custom implementation\n",
        "import torchvision.models as models\n",
        "class MyResNeXt(models.resnet.ResNet):\n",
        "    def __init__(self, training=True):\n",
        "        super(MyResNeXt, self).__init__(block=models.resnet.Bottleneck,\n",
        "                                        layers=[3, 4, 6, 3],\n",
        "                                        groups=32,\n",
        "                                        width_per_group=4)\n",
        "        self.fc = nn.Linear(2048, 1)\n",
        "\n",
        "resnext_model = MyResNeXt().to(device)\n",
        "resnext_model.load_state_dict(torch.load('resnext.pth'))"
      ],
      "metadata": {
        "id": "s55YMNFSmBc-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load dataset\n",
        "test_dataset = UADFVVideoDataset(data_dir=UADFV_dir + '/test', transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Inference on test data\n",
        "def predict_on_test_data():\n",
        "    xception_model.eval()\n",
        "    resnext_model.eval()\n",
        "\n",
        "    for videos, labels in test_loader:\n",
        "        videos = videos.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            xception_output = xception_model(videos)\n",
        "            resnext_output = resnext_model(videos)\n",
        "\n",
        "            # Average the predictions from both models (ensemble)\n",
        "            final_output = (xception_output + resnext_output) / 2\n",
        "            prediction = torch.argmax(final_output, dim=1)\n",
        "\n",
        "            print(f\"Prediction: {'Deepfake' if prediction.item() == 1 else 'Real'}\")\n"
      ],
      "metadata": {
        "id": "wS7QTEaemBc_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuning on the training data\n",
        "train_dataset = UADFVVideoDataset(data_dir=UADFV_dir + '/train', transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Fine-tuning setup\n",
        "for param in xception_model.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in resnext_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze final layers for training\n",
        "for param in xception_model.h1.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in resnext_model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer_xception = optim.Adam(xception_model.h1.parameters(), lr=0.0001)\n",
        "optimizer_resnext = optim.Adam(resnext_model.fc.parameters(), lr=0.0001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    xception_model.train()\n",
        "    resnext_model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for videos, labels in train_loader:\n",
        "        videos, labels = videos.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer_xception.zero_grad()\n",
        "        optimizer_resnext.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        xception_output = xception_model(videos)\n",
        "        resnext_output = resnext_model(videos)\n",
        "\n",
        "        # Average predictions\n",
        "        final_output = (xception_output + resnext_output) / 2\n",
        "        loss = criterion(final_output, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer_xception.step()\n",
        "        optimizer_resnext.step()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        running_loss += loss.item()\n",
        "        predicted = (torch.sigmoid(final_output) > 0.5).float()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# Save the fine-tuned models\n",
        "torch.save(xception_model.state_dict(), 'fine_tuned_xception.pth')\n",
        "torch.save(resnext_model.state_dict(), 'fine_tuned_resnext.pth')\n",
        "\n",
        "# Run predictions on test data\n",
        "predict_on_test_data()"
      ],
      "metadata": {
        "id": "acDAoAbpmBc_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble of Resnext and Xception"
      ],
      "metadata": {
        "id": "vNDMoKLLmBc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df = pd.DataFrame({\"filename\": test_videos})"
      ],
      "metadata": {
        "trusted": true,
        "id": "mQEehl8umBc_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "r1 = 0.46441\n",
        "r2 = 0.52189\n",
        "total = r1 + r2\n",
        "r11 = r1/total\n",
        "r22 = r2/total"
      ],
      "metadata": {
        "trusted": true,
        "id": "OrwUajccmBc_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df[\"label\"] = r22*submission_df_resnext[\"label\"] + r11*submission_df_xception[\"label\"]"
      ],
      "metadata": {
        "trusted": true,
        "id": "w8ZE_PYNmBc_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df.to_csv(\"submission.csv\", index=False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "agMWWfxJmBc_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "from blazeface import BlazeFace\n",
        "from read_video_1 import process_video_frames  # Ensure this script is uploaded\n",
        "\n",
        "# Define UADFV Directory\n",
        "UADFV_dir = \"/kaggle/input/videos\"\n",
        "\n",
        "# Dataset class for UADFV videos\n",
        "class UADFVVideoDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.video_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Load video paths and their corresponding labels\n",
        "        for label in ['real', 'fake']:\n",
        "            label_dir = os.path.join(data_dir, label)\n",
        "            for video_file in os.listdir(label_dir):\n",
        "                self.video_paths.append(os.path.join(label_dir, video_file))\n",
        "                self.labels.append(0 if label == 'real' else 1)  # Assign 0 for real, 1 for fake\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.video_paths[idx]\n",
        "        video_frames = process_video_frames(video_path)  # Extract frames from video\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            video_frames = self.transform(video_frames)\n",
        "\n",
        "        return video_frames, label\n",
        "\n",
        "# Data transformations (if needed)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load BlazeFace Model for face detection\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "facedet = BlazeFace().to(device)\n",
        "facedet.load_weights('blazeface.pth')  # Load BlazeFace weights\n",
        "facedet.load_anchors('anchors.npy')    # Load BlazeFace anchors\n",
        "facedet.train(False)\n",
        "\n",
        "# Load Xception model with your custom implementation\n",
        "from pytorchcv.model_provider import get_model\n",
        "class FCN(nn.Module):\n",
        "    def __init__(self, base, in_f):\n",
        "        super(FCN, self).__init__()\n",
        "        self.base = base\n",
        "        self.h1 = Head(in_f, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base(x)\n",
        "        return self.h1(x)\n",
        "\n",
        "# Load Xception model and remove the output layer\n",
        "xception_base = get_model(\"xception\", pretrained=False)\n",
        "xception_base = nn.Sequential(*list(xception_base.children())[:-1])\n",
        "\n",
        "# Replace final block's pooling layer\n",
        "xception_base[0].final_block.pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)))\n",
        "\n",
        "# Create the full Xception model with a custom head\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, in_f, out_f):\n",
        "        super(Head, self).__init__()\n",
        "        self.f = nn.Flatten()\n",
        "        self.l = nn.Linear(in_f, 512)\n",
        "        self.d = nn.Dropout(0.5)\n",
        "        self.o = nn.Linear(512, out_f)\n",
        "        self.b1 = nn.BatchNorm1d(in_f)\n",
        "        self.b2 = nn.BatchNorm1d(512)\n",
        "        self.r = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.f(x)\n",
        "        x = self.b1(x)\n",
        "        x = self.d(x)\n",
        "        x = self.l(x)\n",
        "        x = self.r(x)\n",
        "        x = self.b2(x)\n",
        "        x = self.d(x)\n",
        "        return self.o(x)\n",
        "\n",
        "xception_model = FCN(xception_base, 2048)\n",
        "xception_model = xception_model.to(device)\n",
        "xception_model.load_state_dict(torch.load('../input/deepfake-xception-trained-model/model.pth'))\n",
        "\n",
        "# Load ResNeXt model with your custom implementation\n",
        "import torchvision.models as models\n",
        "class MyResNeXt(models.resnet.ResNet):\n",
        "    def __init__(self, training=True):\n",
        "        super(MyResNeXt, self).__init__(block=models.resnet.Bottleneck,\n",
        "                                        layers=[3, 4, 6, 3],\n",
        "                                        groups=32,\n",
        "                                        width_per_group=4)\n",
        "        self.fc = nn.Linear(2048, 1)\n",
        "\n",
        "resnext_model = MyResNeXt().to(device)\n",
        "resnext_model.load_state_dict(torch.load('resnext.pth'))\n",
        "\n",
        "# Load dataset\n",
        "test_dataset = UADFVVideoDataset(data_dir=UADFV_dir + '/test', transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Inference on test data\n",
        "def predict_on_test_data():\n",
        "    xception_model.eval()\n",
        "    resnext_model.eval()\n",
        "\n",
        "    for videos, labels in test_loader:\n",
        "        videos = videos.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            xception_output = xception_model(videos)\n",
        "            resnext_output = resnext_model(videos)\n",
        "\n",
        "            # Average the predictions from both models (ensemble)\n",
        "            final_output = (xception_output + resnext_output) / 2\n",
        "            prediction = torch.argmax(final_output, dim=1)\n",
        "\n",
        "            print(f\"Prediction: {'Deepfake' if prediction.item() == 1 else 'Real'}\")\n",
        "\n",
        "# Fine-tuning on the training data\n",
        "train_dataset = UADFVVideoDataset(data_dir=UADFV_dir + '/train', transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Fine-tuning setup\n",
        "for param in xception_model.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in resnext_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze final layers for training\n",
        "for param in xception_model.h1.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in resnext_model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer_xception = optim.Adam(xception_model.h1.parameters(), lr=0.0001)\n",
        "optimizer_resnext = optim.Adam(resnext_model.fc.parameters(), lr=0.0001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    xception_model.train()\n",
        "    resnext_model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for videos, labels in train_loader:\n",
        "        videos, labels = videos.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer_xception.zero_grad()\n",
        "        optimizer_resnext.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        xception_output = xception_model(videos)\n",
        "        resnext_output = resnext_model(videos)\n",
        "\n",
        "        # Average predictions\n",
        "        final_output = (xception_output + resnext_output) / 2\n",
        "        loss = criterion(final_output, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer_xception.step()\n",
        "        optimizer_resnext.step()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        running_loss += loss.item()\n",
        "        predicted = (torch.sigmoid(final_output) > 0.5).float()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# Save the fine-tuned models\n",
        "torch.save(xception_model.state_dict(), 'fine_tuned_xception.pth')\n",
        "torch.save(resnext_model.state_dict(), 'fine_tuned_resnext.pth')\n",
        "\n",
        "# Run predictions on test data\n",
        "predict_on_test_data()\n"
      ],
      "metadata": {
        "id": "SVqh5YBDmBc_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import os\n",
        "from blazeface import BlazeFace\n",
        "from helpers.read_video_1 import VideoReader\n",
        "from helpers.face_extract_1 import FaceExtractor\n",
        "\n",
        "# Define UADFV Directory\n",
        "UADFV_dir = \"/kaggle/input/videos\"\n",
        "\n",
        "# Initialize BlazeFace\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "facedet = BlazeFace().to(device)\n",
        "facedet.load_weights(\"/kaggle/input/blazeface-pytorch/blazeface.pth\")\n",
        "facedet.load_anchors(\"/kaggle/input/blazeface-pytorch/anchors.npy\")\n",
        "facedet.train(False)\n",
        "\n",
        "frames_per_video = 64\n",
        "video_reader = VideoReader()\n",
        "video_read_fn = lambda x: video_reader.read_frames(x, num_frames=frames_per_video)\n",
        "face_extractor = FaceExtractor(video_read_fn, facedet)\n",
        "\n",
        "# Custom transformations\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "normalize_transform = transforms.Normalize(mean, std)\n",
        "\n",
        "# Helper functions to resize images\n",
        "def isotropically_resize_image(img, size, resample=cv2.INTER_AREA):\n",
        "    h, w = img.shape[:2]\n",
        "    if w > h:\n",
        "        h = h * size // w\n",
        "        w = size\n",
        "    else:\n",
        "        w = w * size // h\n",
        "        h = size\n",
        "    resized = cv2.resize(img, (w, h), interpolation=resample)\n",
        "    return resized\n",
        "\n",
        "def make_square_image(img):\n",
        "    h, w = img.shape[:2]\n",
        "    size = max(h, w)\n",
        "    t = 0\n",
        "    b = size - h\n",
        "    l = 0\n",
        "    r = size - w\n",
        "    return cv2.copyMakeBorder(img, t, b, l, r, cv2.BORDER_CONSTANT, value=0)\n",
        "\n",
        "# Dataset class for UADFV videos\n",
        "class UADFVVideoDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.video_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Load video paths and their corresponding labels\n",
        "        for label in ['real', 'fake']:\n",
        "            label_dir = os.path.join(data_dir, label)\n",
        "            for video_file in os.listdir(label_dir):\n",
        "                self.video_paths.append(os.path.join(label_dir, video_file))\n",
        "                self.labels.append(0 if label == 'real' else 1)  # Assign 0 for real, 1 for fake\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.video_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        print (video_path,label)\n",
        "        return video_path, label\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-20T02:17:09.413316Z",
          "iopub.execute_input": "2024-10-20T02:17:09.413703Z",
          "iopub.status.idle": "2024-10-20T02:17:09.459197Z",
          "shell.execute_reply.started": "2024-10-20T02:17:09.413625Z",
          "shell.execute_reply": "2024-10-20T02:17:09.458618Z"
        },
        "id": "3jxGImDFmBc_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ../input/deepfake-xception-trained-model/pytorchcv-0.0.55-py2.py3-none-any.whl --quiet"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-20T00:51:18.672867Z",
          "iopub.execute_input": "2024-10-20T00:51:18.673314Z",
          "iopub.status.idle": "2024-10-20T00:51:46.716236Z",
          "shell.execute_reply.started": "2024-10-20T00:51:18.673119Z",
          "shell.execute_reply": "2024-10-20T00:51:46.715167Z"
        },
        "id": "I_lMKRQfmBdA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_video(video_frames):\n",
        "    \"\"\"\n",
        "    Preprocess a batch of video frames:\n",
        "\n",
        "Resize\n",
        "Normalize\n",
        "Convert to Tensor\n",
        "\n",
        "    Args:\n",
        "    video_frames (list or ndarray): List of frames (could be loaded from a video).\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: Preprocessed video frames ready for model input.\n",
        "    \"\"\"\n",
        "\n",
        "    # Assuming you need to resize to a specific size, like (224, 224)\n",
        "    resize_transform = transforms.Resize((224, 224))\n",
        "\n",
        "    # Normalize based on the model's requirements (mean and std from the dataset)\n",
        "    normalize_transform = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    # ToTensor converts the images to PyTorch tensors\n",
        "    preprocess_transform = transforms.Compose([\n",
        "        resize_transform,\n",
        "        transforms.ToTensor(),\n",
        "        normalize_transform\n",
        "    ])\n",
        "\n",
        "    # Apply the transform to each frame in the video\n",
        "    preprocessed_frames = [preprocess_transform(frame) for frame in video_frames]\n",
        "\n",
        "    # Stack the frames along the batch dimension to create a single tensor\n",
        "    video_tensor = torch.stack(preprocessed_frames)\n",
        "\n",
        "    return video_tensor"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-20T02:17:12.567623Z",
          "iopub.execute_input": "2024-10-20T02:17:12.567911Z",
          "iopub.status.idle": "2024-10-20T02:17:12.575058Z",
          "shell.execute_reply.started": "2024-10-20T02:17:12.567870Z",
          "shell.execute_reply": "2024-10-20T02:17:12.574263Z"
        },
        "id": "Z3WiV3n0mBdA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input sizes for each model\n",
        "input_size_resnext = 224  # ResNeXt input size\n",
        "input_size_xception = 150  # XceptionNet input size\n",
        "\n",
        "# Helper function to process faces for both models\n",
        "def predict_on_video(video_path, batch_size):\n",
        "    try:\n",
        "        # Find the faces for N frames in the video.\n",
        "        faces = face_extractor.process_video(video_path)\n",
        "\n",
        "        # Only look at one face per frame.\n",
        "        face_extractor.keep_only_best_face(faces)\n",
        "\n",
        "        if len(faces) > 0:\n",
        "            # Prepare inputs for ResNeXt and Xception\n",
        "            x_resnext = np.zeros((batch_size, input_size_resnext, input_size_resnext, 3), dtype=np.uint8)\n",
        "            x_xception = np.zeros((batch_size, input_size_xception, input_size_xception, 3), dtype=np.uint8)\n",
        "\n",
        "            # If we found any faces, resize them for each model.\n",
        "            n = 0\n",
        "            for frame_data in faces:\n",
        "                for face in frame_data[\"faces\"]:\n",
        "                    # Resize for ResNeXt\n",
        "                    resized_face_resnext = isotropically_resize_image(face, input_size_resnext)\n",
        "                    resized_face_resnext = make_square_image(resized_face_resnext)\n",
        "                    # Resize for Xception\n",
        "                    resized_face_xception = isotropically_resize_image(face, input_size_xception)\n",
        "                    resized_face_xception = make_square_image(resized_face_xception)\n",
        "\n",
        "                    if n < batch_size:\n",
        "                        x_resnext[n] = resized_face_resnext\n",
        "                        x_xception[n] = resized_face_xception\n",
        "                        n += 1\n",
        "\n",
        "            if n > 0:\n",
        "                x_resnext = torch.tensor(x_resnext, device=device).float().permute((0, 3, 1, 2))\n",
        "                x_xception = torch.tensor(x_xception, device=device).float().permute((0, 3, 1, 2))\n",
        "\n",
        "                # Preprocess the images (normalize them)\n",
        "                for i in range(len(x_resnext)):\n",
        "                    x_resnext[i] = normalize_transform(x_resnext[i] / 255.)\n",
        "                for i in range(len(x_xception)):\n",
        "                    x_xception[i] = normalize_transform(x_xception[i] / 255.)\n",
        "\n",
        "                # Make predictions for both models and return the average\n",
        "                with torch.no_grad():\n",
        "                    y_pred_resnext = resnext_model(x_resnext)\n",
        "                    y_pred_xception = xception_model(x_xception)\n",
        "                    y_pred_resnext = torch.sigmoid(y_pred_resnext.squeeze())\n",
        "                    y_pred_xception = torch.sigmoid(y_pred_xception.squeeze())\n",
        "\n",
        "                    return (y_pred_resnext[:n].mean().item() + y_pred_xception[:n].mean().item()) / 2\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Prediction error on video {video_path}: {str(e)}\")\n",
        "\n",
        "    return 0.5"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-20T02:47:44.837161Z",
          "iopub.execute_input": "2024-10-20T02:47:44.837556Z",
          "iopub.status.idle": "2024-10-20T02:47:44.859440Z",
          "shell.execute_reply.started": "2024-10-20T02:47:44.837497Z",
          "shell.execute_reply": "2024-10-20T02:47:44.858552Z"
        },
        "id": "XXbnUsiYmBdA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load models\n",
        "from pytorchcv.model_provider import get_model\n",
        "\n",
        "# Xception model\n",
        "class FCN(nn.Module):\n",
        "    def __init__(self, base, in_f):\n",
        "        super(FCN, self).__init__()\n",
        "        self.base = base\n",
        "        self.h1 = Head(in_f, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base(x)\n",
        "        return self.h1(x)\n",
        "\n",
        "# Custom head for Xception\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, in_f, out_f):\n",
        "        super(Head, self).__init__()\n",
        "        self.f = nn.Flatten()\n",
        "        self.l = nn.Linear(in_f, 512)\n",
        "        self.d = nn.Dropout(0.5)\n",
        "        self.o = nn.Linear(512, out_f)\n",
        "        self.b1 = nn.BatchNorm1d(in_f)\n",
        "        self.b2 = nn.BatchNorm1d(512)\n",
        "        self.r = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.f(x)\n",
        "        x = self.b1(x)\n",
        "        x = self.d(x)\n",
        "        x = self.l(x)\n",
        "        x = self.r(x)\n",
        "        x = self.b2(x)\n",
        "        x = self.d(x)\n",
        "        return self.o(x)\n",
        "\n",
        "xception_base = get_model(\"xception\", pretrained=False)\n",
        "xception_base = nn.Sequential(*list(xception_base.children())[:-1])\n",
        "xception_base[0].final_block.pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)))\n",
        "\n",
        "xception_model = FCN(xception_base, 2048).to(device)\n",
        "xception_model.load_state_dict(torch.load('../input/deepfake-xception-trained-model/model.pth'))\n",
        "\n",
        "# ResNeXt model\n",
        "import torchvision.models as models\n",
        "class MyResNeXt(models.resnet.ResNet):\n",
        "    def __init__(self, training=True):\n",
        "        super(MyResNeXt, self).__init__(block=models.resnet.Bottleneck,\n",
        "                                        layers=[3, 4, 6, 3],\n",
        "                                        groups=32,\n",
        "                                        width_per_group=4)\n",
        "        self.fc = nn.Linear(2048, 1)\n",
        "\n",
        "resnext_model = MyResNeXt().to(device)\n",
        "resnext_model.load_state_dict(torch.load('/kaggle/input/deepfakes-inference-demo/resnext.pth'))\n",
        "\n",
        "# Load dataset\n",
        "test_dataset = UADFVVideoDataset(data_dir=UADFV_dir + '/test', transform=None)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Prediction on test data\n",
        "def predict_on_test_data():\n",
        "    xception_model.eval()\n",
        "    resnext_model.eval()\n",
        "\n",
        "    for videos, labels in test_loader:\n",
        "        videos = videos.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            xception_output = xception_model(videos)\n",
        "            resnext_output = resnext_model(videos)\n",
        "\n",
        "            # Ensemble predictions\n",
        "            final_output = (xception_output + resnext_output) / 2\n",
        "            prediction = torch.argmax(final_output, dim=1)\n",
        "\n",
        "            print(f\"Prediction: {'Deepfake' if prediction.item() == 1 else 'Real'}\")\n",
        "\n",
        "# Speed test on videos\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import time\n",
        "\n",
        "def predict_on_video_paths(video_paths, num_workers):\n",
        "    def process_file(video_path):\n",
        "        y_pred = predict_on_video(video_path, batch_size=frames_per_video)\n",
        "        return y_pred\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as ex:\n",
        "        predictions = ex.map(process_file, video_paths)\n",
        "\n",
        "    return list(predictions)\n",
        "def predict_on_data_loader(dataloader, batch_size):\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    for video_path, label in dataloader:\n",
        "        print(f\"Video path type: {type(video_path)}\")\n",
        "        print(f\"Video label: {type(label)}\")\n",
        "        # Unpack the video path if it's inside a tuple\n",
        "        if isinstance(video_path, tuple):\n",
        "            video_path = video_path[0]\n",
        "\n",
        "        # Call the predict_on_video function for each video path\n",
        "        prediction = predict_on_video(video_path, batch_size)\n",
        "        all_predictions.append(prediction)\n",
        "        all_labels.append(label)\n",
        "\n",
        "    return all_predictions, all_labels\n",
        "#Run predictions on a small batch (using file paths)\n",
        "speed_test = True\n",
        "if speed_test:\n",
        "    start_time = time.time()\n",
        "    speedtest_videos = ['/kaggle/input/videos/test/fake/0046_fake.mp4', '/kaggle/input/videos/test/real/0046.mp4']  # Adjust your video paths\n",
        "    predictions = predict_on_video_paths(speedtest_videos, num_workers=4)\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"Elapsed {elapsed:.4f} sec. Average per video: {elapsed / len(speedtest_videos):.4f} sec.\")\n",
        "    print(predictions)\n",
        "#Inference on full test set (using DataLoader)\n",
        "xception_model.eval()\n",
        "resnext_model.eval()\n",
        "\n",
        "predictions = predict_on_data_loader(test_loader,4)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-20T02:47:51.445799Z",
          "iopub.execute_input": "2024-10-20T02:47:51.446093Z",
          "iopub.status.idle": "2024-10-20T02:47:57.662217Z",
          "shell.execute_reply.started": "2024-10-20T02:47:51.446051Z",
          "shell.execute_reply": "2024-10-20T02:47:57.661522Z"
        },
        "id": "lTPZ9OZTmBdA",
        "outputId": "25194e34-3d46-4462-cead-9d0435da8f8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Elapsed 1.4304 sec. Average per video: 0.7152 sec.\n[0.48261022567749023, 0.45802871882915497]\n/kaggle/input/videos/test/real/0046.mp4 0\nVideo path type: <class 'tuple'>\nVideo label: <class 'torch.Tensor'>\n/kaggle/input/videos/test/real/0047.mp4 0\nVideo path type: <class 'tuple'>\nVideo label: <class 'torch.Tensor'>\n/kaggle/input/videos/test/real/0048.mp4 0\nVideo path type: <class 'tuple'>\nVideo label: <class 'torch.Tensor'>\n/kaggle/input/videos/test/fake/0046_fake.mp4 1\nVideo path type: <class 'tuple'>\nVideo label: <class 'torch.Tensor'>\n/kaggle/input/videos/test/fake/0048_fake.mp4 1\nVideo path type: <class 'tuple'>\nVideo label: <class 'torch.Tensor'>\n/kaggle/input/videos/test/fake/0047_fake.mp4 1\nVideo path type: <class 'tuple'>\nVideo label: <class 'torch.Tensor'>\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print (predictions)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-20T02:48:15.224709Z",
          "iopub.execute_input": "2024-10-20T02:48:15.225038Z",
          "iopub.status.idle": "2024-10-20T02:48:15.230254Z",
          "shell.execute_reply.started": "2024-10-20T02:48:15.224981Z",
          "shell.execute_reply": "2024-10-20T02:48:15.229542Z"
        },
        "id": "-KN_WnIGmBdA",
        "outputId": "c4da627c-d0fc-4bf5-cefd-be5cd4636051"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "([0.4993215650320053, 0.0613549305126071, 0.7446506321430206, 0.6337763518095016, 0.9603795409202576, 0.5451347678899765], [tensor([0]), tensor([0]), tensor([0]), tensor([1]), tensor([1]), tensor([1])])\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#Save ResNeXt model\n",
        "torch.save(resnext_model.state_dict(), 'resnext_model.pth')\n",
        "#Save Xception model\n",
        "torch.save(xception_model.state_dict(), 'xception_model.pth')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-20T02:30:26.984655Z",
          "iopub.execute_input": "2024-10-20T02:30:26.985112Z",
          "iopub.status.idle": "2024-10-20T02:30:27.209968Z",
          "shell.execute_reply.started": "2024-10-20T02:30:26.984933Z",
          "shell.execute_reply": "2024-10-20T02:30:27.209292Z"
        },
        "id": "RfbBa5PzmBdB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_dir = \"/kaggle/input/deepfake-detection-challenge/test_videos/\"\n",
        "\n",
        "test_videos = sorted([x for x in os.listdir(test_dir) if x[-4:] == \".mp4\"])\n",
        "frame_h = 5\n",
        "frame_l = 5\n",
        "len(test_videos)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-20T02:44:07.307069Z",
          "iopub.execute_input": "2024-10-20T02:44:07.307522Z",
          "iopub.status.idle": "2024-10-20T02:44:07.317410Z",
          "shell.execute_reply.started": "2024-10-20T02:44:07.307452Z",
          "shell.execute_reply": "2024-10-20T02:44:07.316675Z"
        },
        "id": "vX2LcCWImBdB",
        "outputId": "90a725c9-9807-4499-b769-37c28c0c253f"
      },
      "outputs": [
        {
          "execution_count": 52,
          "output_type": "execute_result",
          "data": {
            "text/plain": "400"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = predict_on_video_paths(test_videos, num_workers=4)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-10-20T02:44:12.098428Z",
          "iopub.execute_input": "2024-10-20T02:44:12.098723Z",
          "iopub.status.idle": "2024-10-20T02:44:12.188918Z",
          "shell.execute_reply.started": "2024-10-20T02:44:12.098681Z",
          "shell.execute_reply": "2024-10-20T02:44:12.187693Z"
        },
        "id": "1wb2vJZMmBdB",
        "outputId": "7fd347a0-3433-42de-cff4-1dd7e9d9cf50"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Prediction error on video aassnaulhq.mp4: need at least one array to concatenate\nPrediction error on video aayfryxljh.mp4: need at least one array to concatenate\nPrediction error on video acazlolrpz.mp4: need at least one array to concatenate\nPrediction error on video adohdulfwb.mp4: need at least one array to concatenate\nPrediction error on video aktnlyqpah.mp4: need at least one array to concatenate\nPrediction error on video ahjnxtiamx.mp4: need at least one array to concatenate\nPrediction error on video alrtntfxtd.mp4: need at least one array to concatenate\nPrediction error on video apedduehoy.mp4: need at least one array to concatenate\nPrediction error on video ajiyrjfyzp.mp4: need at least one array to concatenatePrediction error on video apvzjkvnwn.mp4: need at least one array to concatenate\nPrediction error on video aomqqjipcp.mp4: need at least one array to concatenate\nPrediction error on video aqrsylrzgi.mp4: need at least one array to concatenate\nPrediction error on video ayipraspbn.mp4: need at least one array to concatenate\nPrediction error on video axfhbpkdlc.mp4: need at least one array to concatenatePrediction error on video bcbqxhziqz.mp4: need at least one array to concatenate\n\nPrediction error on video bdshuoldwx.mp4: need at least one array to concatenate\nPrediction error on video bcvheslzrq.mp4: need at least one array to concatenate\nPrediction error on video bfdopzvxbi.mp4: need at least one array to concatenate\nPrediction error on video bjyaxvggle.mp4: need at least one array to concatenate\nPrediction error on video bkcyglmfci.mp4: need at least one array to concatenate\nPrediction error on video bktkwbcawi.mp4: need at least one array to concatenate\nPrediction error on video bkuzquigyt.mp4: need at least one array to concatenate\n\nPrediction error on video blnmxntbey.mp4: need at least one array to concatenate\nPrediction error on video bfjsthfhbd.mp4: need at least one array to concatenate\nPrediction error on video bofrwgeyjo.mp4: need at least one array to concatenate\nPrediction error on video blszgmxkvu.mp4: need at least one array to concatenatePrediction error on video btdxnajogv.mp4: need at least one array to concatenate\n\nPrediction error on video bvpeerislp.mp4: need at least one array to concatenatePrediction error on video bzvzpwrabw.mp4: need at least one array to concatenate\nPrediction error on video bwdmzwhdnw.mp4: need at least one array to concatenate\nPrediction error on video cekarydqba.mp4: need at least one array to concatenate\n\nPrediction error on video bnuwxhfahw.mp4: need at least one array to concatenate\nPrediction error on video cjkctqqakb.mp4: need at least one array to concatenate\nPrediction error on video cekwtyxdoo.mp4: need at least one array to concatenatePrediction error on video cnxccbjlct.mp4: need at least one array to concatenate\nPrediction error on video cnpanmywno.mp4: need at least one array to concatenatePrediction error on video cosghhimnd.mp4: need at least one array to concatenate\nPrediction error on video coujjnypba.mp4: need at least one array to concatenate\nPrediction error on video coqwgzpbhx.mp4: need at least one array to concatenate\n\nPrediction error on video cqhwesrciw.mp4: need at least one array to concatenate\nPrediction error on video csnkohqxdv.mp4: need at least one array to concatenate\nPrediction error on video cqxxumarvp.mp4: need at least one array to concatenatePrediction error on video czfqlbcfpa.mp4: need at least one array to concatenate\n\n\nPrediction error on video ddtbarpcgo.mp4: need at least one array to concatenate\nPrediction error on video cxsvvnxpyz.mp4: need at least one array to concatenate\nPrediction error on video demuhxssgl.mp4: need at least one array to concatenate\nPrediction error on video didzujjhtg.mp4: need at least one array to concatenatePrediction error on video dkuqbduxev.mp4: need at least one array to concatenate\nPrediction error on video dcqodpzomd.mp4: need at least one array to concatenate\nPrediction error on video dmmvuaikkv.mp4: need at least one array to concatenate\nPrediction error on video doniqevxeg.mp4: need at least one array to concatenate\n\nPrediction error on video dnmowthjcj.mp4: need at least one array to concatenate\nPrediction error on video dpmgoiwhuf.mp4: need at least one array to concatenate\nPrediction error on video dpevefkefv.mp4: need at least one array to concatenate\nPrediction error on video dozjwhnedd.mp4: need at least one array to concatenate\nPrediction error on video dsnxgrfdmd.mp4: need at least one array to concatenate\nPrediction error on video dvtpwatuja.mp4: need at least one array to concatenate\nPrediction error on video dtozwcapoa.mp4: need at least one array to concatenatePrediction error on video dxfdovivlw.mp4: need at least one array to concatenate\nPrediction error on video dvkdfhrpph.mp4: need at least one array to concatenate\nPrediction error on video dvwpvqdflx.mp4: need at least one array to concatenatePrediction error on video dxgnpnowgk.mp4: need at least one array to concatenate\nPrediction error on video dyjklprkoc.mp4: need at least one array to concatenate\n\nPrediction error on video dzojiwfvba.mp4: need at least one array to concatenate\nPrediction error on video ecumyiowzs.mp4: need at least one array to concatenate\nPrediction error on video eisofhptvk.mp4: need at least one array to concatenate\nPrediction error on video ekboxwrwuv.mp4: need at least one array to concatenate\n\nPrediction error on video ekelfsnqof.mp4: need at least one array to concatenate\nPrediction error on video ekvwecwltj.mp4: need at least one array to concatenate\nPrediction error on video dzkyxbbqkr.mp4: need at least one array to concatenate\nPrediction error on video eqslzbqfea.mp4: need at least one array to concatenate\nPrediction error on video eppyqpgewp.mp4: need at least one array to concatenate\nPrediction error on video elackxuccp.mp4: need at least one array to concatenate\nPrediction error on video eryjktdexi.mp4: need at least one array to concatenate\nPrediction error on video espkiocpxq.mp4: need at least one array to concatenate\nPrediction error on video esjdyghhog.mp4: need at least one array to concatenate\nPrediction error on video esmqxszybs.mp4: need at least one array to concatenate\nPrediction error on video etdliwticv.mp4: need at least one array to concatenate\nPrediction error on video eyguqfmgzh.mp4: need at least one array to concatenatePrediction error on video evysmtpnrf.mp4: need at least one array to concatenate\n\nPrediction error on video fddmkqjwsh.mp4: need at least one array to concatenate\nPrediction error on video famlupsgqm.mp4: need at least one array to concatenate\nPrediction error on video eywdmustbb.mp4: need at least one array to concatenate\nPrediction error on video fmvvmcbdrw.mp4: need at least one array to concatenate\nPrediction error on video fjxovgmwnm.mp4: need at least one array to concatenate\nPrediction error on video fneqiqpqvs.mp4: need at least one array to concatenate\nPrediction error on video fmhiujydwo.mp4: need at least one array to concatenatePrediction error on video fjrueenjyp.mp4: need at least one array to concatenate\nPrediction error on video fopjiyxiqd.mp4: need at least one array to concatenate\nPrediction error on video fnxgqcvlsd.mp4: need at least one array to concatenate\n\nPrediction error on video frqfsucgao.mp4: need at least one array to concatenate\nPrediction error on video fpevfidstw.mp4: need at least one array to concatenate\nPrediction error on video fqgypsunzr.mp4: need at least one array to concatenate\nPrediction error on video fsdrwikhge.mp4: need at least one array to concatenate\nPrediction error on video fwykevubzy.mp4: need at least one array to concatenate\nPrediction error on video fxuxxtryjn.mp4: need at least one array to concatenate\nPrediction error on video gahgyuwzbu.mp4: need at least one array to concatenate\nPrediction error on video fzvpbrzssi.mp4: need at least one array to concatenate\nPrediction error on video gcdtglsoqj.mp4: need at least one array to concatenate\nPrediction error on video gbnzicjyhz.mp4: need at least one array to concatenate\nPrediction error on video gfdjzwnpyp.mp4: need at least one array to concatenate\nPrediction error on video gccnvdoknm.mp4: need at least one array to concatenate\nPrediction error on video ggdpclfcgk.mp4: need at least one array to concatenate\nPrediction error on video ghnpsltzyn.mp4: need at least one array to concatenatePrediction error on video ggzjfrirjh.mp4: need at least one array to concatenate\n\nPrediction error on video gfgcwxkbjd.mp4: need at least one array to concatenatePrediction error on video gfcycflhbo.mp4: need at least one array to concatenate\nPrediction error on video gkutjglghz.mp4: need at least one array to concatenate\n\nPrediction error on video gochxzemmq.mp4: need at least one array to concatenate\nPrediction error on video gpsxfxrjrr.mp4: need at least one array to concatenate\nPrediction error on video gqnaxievjx.mp4: need at least one array to concatenate\nPrediction error on video gunamloolc.mp4: need at least one array to concatenatePrediction error on video halvwiltfs.mp4: need at least one array to concatenate\nPrediction error on video hbufmvbium.mp4: need at least one array to concatenate\nPrediction error on video hefisnapds.mp4: need at least one array to concatenate\nPrediction error on video hcanfkwivl.mp4: need at least one array to concatenate\n\nPrediction error on video heiyoojifp.mp4: need at least one array to concatenate\nPrediction error on video hfsvqabzfq.mp4: need at least one array to concatenate\nPrediction error on video hclsparpth.mp4: need at least one array to concatenate\nPrediction error on video hevcclcklc.mp4: need at least one array to concatenatePrediction error on video hitfycdavv.mp4: need at least one array to concatenate\nPrediction error on video hicjuubiau.mp4: need at least one array to concatenatePrediction error on video hierggamuo.mp4: need at least one array to concatenate\nPrediction error on video hnfwagcxdf.mp4: need at least one array to concatenate\n\nPrediction error on video honxqdilvv.mp4: need at least one array to concatenate\n\nPrediction error on video hsbwhlolsn.mp4: need at least one array to concatenate\nPrediction error on video hszwwswewp.mp4: need at least one array to concatenate\nPrediction error on video hsbljbsgxr.mp4: need at least one array to concatenate\nPrediction error on video hqzwudvhih.mp4: need at least one array to concatenate\nPrediction error on video huvlwkxoxm.mp4: need at least one array to concatenatePrediction error on video hweshqpfwe.mp4: need at least one array to concatenate\n\nPrediction error on video hyjqolupxn.mp4: need at least one array to concatenate\nPrediction error on video htzbnroagi.mp4: need at least one array to concatenate\nPrediction error on video hxwtsaydal.mp4: need at least one array to concatenate\nPrediction error on video ibxfxggtqh.mp4: need at least one array to concatenate\nPrediction error on video hzssdinxec.mp4: need at least one array to concatenate\nPrediction error on video hzoiotcykp.mp4: need at least one array to concatenate\nPrediction error on video ihglzxzroo.mp4: need at least one array to concatenate\nPrediction error on video icbsahlivv.mp4: need at least one array to concatenate\nPrediction error on video iksxzpqxzi.mp4: need at least one array to concatenate\nPrediction error on video igpvrfjdzc.mp4: need at least one array to concatenate\nPrediction error on video ilqwcbprqa.mp4: need at least one array to concatenate\nPrediction error on video imdmhwkkni.mp4: need at least one array to concatenate\nPrediction error on video ipkpxvwroe.mp4: need at least one array to concatenate\nPrediction error on video iorbtaarte.mp4: need at least one array to concatenatePrediction error on video itfsvvmslp.mp4: need at least one array to concatenate\nPrediction error on video irqzdokcws.mp4: need at least one array to concatenate\nPrediction error on video iznnzjvaxc.mp4: need at least one array to concatenate\nPrediction error on video ipvwtgdlre.mp4: need at least one array to concatenate\nPrediction error on video jawgcggquk.mp4: need at least one array to concatenate\nPrediction error on video jhczqfefgw.mp4: need at least one array to concatenate\nPrediction error on video jiavqbrkyk.mp4: need at least one array to concatenate\nPrediction error on video jiswxuqzyz.mp4: need at least one array to concatenate\nPrediction error on video jquevmhdvc.mp4: need at least one array to concatenate\nPrediction error on video jsbpkpxwew.mp4: need at least one array to concatenate\nPrediction error on video jyfvaequfg.mp4: need at least one array to concatenate\n\nPrediction error on video jsysgmycsx.mp4: need at least one array to concatenate\nPrediction error on video jytrvwlewz.mp4: need at least one array to concatenate\nPrediction error on video kcjvhgvhpt.mp4: need at least one array to concatenate\nPrediction error on video jzmzdispyo.mp4: need at least one array to concatenate\nPrediction error on video jyoxdvxpza.mp4: need at least one array to concatenate\nPrediction error on video kezwvsxxzj.mp4: need at least one array to concatenate\nPrediction error on video keioymnobc.mp4: need at least one array to concatenate\nPrediction error on video khpipxnsvx.mp4: need at least one array to concatenate\nPrediction error on video kmqkiihrmj.mp4: need at least one array to concatenate\nPrediction error on video kmcdjxmnoa.mp4: need at least one array to concatenate\nPrediction error on video knxltsvzyu.mp4: need at least one array to concatenate\nPrediction error on video kowiwvrjht.mp4: need at least one array to concatenatePrediction error on video kqlvggiqee.mp4: need at least one array to concatenate\nPrediction error on video kwfdyqofzw.mp4: need at least one array to concatenate\nPrediction error on video lbfqksftuo.mp4: need at least one array to concatenate\n\nPrediction error on video kvmpmhdxly.mp4: need at least one array to concatenate\nPrediction error on video lebzjtusnr.mp4: need at least one array to concatenate\nPrediction error on video lbigytrrtr.mp4: need at least one array to concatenate\nPrediction error on video ljouzjaqqe.mp4: need at least one array to concatenate\nPrediction error on video ljauauuyka.mp4: need at least one array to concatenate\nPrediction error on video lhvjzhjxdp.mp4: need at least one array to concatenate\nPrediction error on video llplvmcvbl.mp4: need at least one array to concatenate\nPrediction error on video lmdyicksrv.mp4: need at least one array to concatenate\nPrediction error on video lnjkpdviqb.mp4: need at least one array to concatenatePrediction error on video lpkgabskbw.mp4: need at least one array to concatenate\n\nPrediction error on video lnhkjhyhvw.mp4: need at least one array to concatenate\nPrediction error on video lpgxwdgnio.mp4: need at least one array to concatenate\nPrediction error on video lujvyveojc.mp4: need at least one array to concatenate\nPrediction error on video mkzaekkvej.mp4: need at least one array to concatenate\nPrediction error on video lyoslorecs.mp4: need at least one array to concatenate\nPrediction error on video mkmgcxaztt.mp4: need at least one array to concatenate\nPrediction error on video mdfndlljvt.mp4: need at least one array to concatenatePrediction error on video mllzkpgatp.mp4: need at least one array to concatenate\n\nPrediction error on video mnzabbkpmt.mp4: need at least one array to concatenate\nPrediction error on video mnowxangqx.mp4: need at least one array to concatenate\nPrediction error on video mwnibuujwz.mp4: need at least one array to concatenate\nPrediction error on video mohiqoogpb.mp4: need at least one array to concatenate\nPrediction error on video mwwploizlj.mp4: need at least one array to concatenate\nPrediction error on video mxahsihabr.mp4: need at least one array to concatenate\nPrediction error on video mszblrdprw.mp4: need at least one array to concatenate\nPrediction error on video ncoeewrdlo.mp4: need at least one array to concatenate\nPrediction error on video ncmpqwmnzb.mp4: need at least one array to concatenatePrediction error on video ndikguxzek.mp4: need at least one array to concatenate\nPrediction error on video nikynwcvuh.mp4: need at least one array to concatenate\nPrediction error on video mxlipjhmqk.mp4: need at least one array to concatenatePrediction error on video nkhzxomani.mp4: need at least one array to concatenate\nPrediction error on video njzshtfmcw.mp4: need at least one array to concatenate\n\n\nPrediction error on video nswtvttxre.mp4: need at least one array to concatenate\nPrediction error on video nplviymzlg.mp4: need at least one array to concatenatePrediction error on video nwvloufjty.mp4: need at least one array to concatenate\nPrediction error on video nthpnwylxo.mp4: need at least one array to concatenate\nPrediction error on video novarhxpbj.mp4: need at least one array to concatenate\nPrediction error on video nxgzmgzkfv.mp4: need at least one array to concatenate\n\nPrediction error on video nxnmkytwze.mp4: need at least one array to concatenatePrediction error on video nycmyuzpml.mp4: need at least one array to concatenate\n\nPrediction error on video nymodlmxni.mp4: need at least one array to concatenatePrediction error on video nxzgekegsp.mp4: need at least one array to concatenate\n\nPrediction error on video oaguiggjyv.mp4: need at least one array to concatenatePrediction error on video oefukgnvel.mp4: need at least one array to concatenate\nPrediction error on video oelqpetgwj.mp4: need at least one array to concatenate\nPrediction error on video nwvsbmyndn.mp4: need at least one array to concatenate\n\nPrediction error on video ojsxxkalat.mp4: need at least one array to concatenate\nPrediction error on video ocgdbrgmtq.mp4: need at least one array to concatenate\nPrediction error on video okgelildpc.mp4: need at least one array to concatenate\nPrediction error on video omphqltjdd.mp4: need at least one array to concatenate\nPrediction error on video ooafcxxfrs.mp4: need at least one array to concatenatePrediction error on video oocincvedt.mp4: need at least one array to concatenatePrediction error on video opvqdabdap.mp4: need at least one array to concatenate\nPrediction error on video oojxonbgow.mp4: need at least one array to concatenate\n\n\nPrediction error on video orixbcfvdz.mp4: need at least one array to concatenate\nPrediction error on video orekjthsef.mp4: need at least one array to concatenate\nPrediction error on video owaogcehvc.mp4: need at least one array to concatenate\nPrediction error on video oyqgwjdwaj.mp4: need at least one array to concatenate\nPrediction error on video oysopgovhu.mp4: need at least one array to concatenatePrediction error on video pcoxcmtroa.mp4: need at least one array to concatenate\nPrediction error on video ouaowjmigq.mp4: need at least one array to concatenate\nPrediction error on video papagllumt.mp4: need at least one array to concatenate\nPrediction error on video pdswwyyntw.mp4: need at least one array to concatenate\n\nPrediction error on video pdufsewrec.mp4: need at least one array to concatenate\nPrediction error on video pcyswtgick.mp4: need at least one array to concatenate\nPrediction error on video pqdeutauqc.mp4: need at least one array to concatenatePrediction error on video pqthmvwonf.mp4: need at least one array to concatenate\n\nPrediction error on video phjvutxpoi.mp4: need at least one array to concatenate\nPrediction error on video petmyhjclt.mp4: need at least one array to concatenate\nPrediction error on video prhmixykhr.mp4: need at least one array to concatenate\nPrediction error on video prwsfljdjo.mp4: need at least one array to concatenate\nPrediction error on video psesikjaxx.mp4: need at least one array to concatenate\nPrediction error on video ptbfnkajyi.mp4: need at least one array to concatenate\nPrediction error on video pxcfrszlgi.mp4: need at least one array to concatenate\nPrediction error on video pxjkzvqomp.mp4: need at least one array to concatenate\nPrediction error on video qarqtkvgby.mp4: need at least one array to concatenate\nPrediction error on video qcbkztamqc.mp4: need at least one array to concatenate\nPrediction error on video ptbnewtvon.mp4: need at least one array to concatenate\nPrediction error on video qclpbcbgeq.mp4: need at least one array to concatenatePrediction error on video qhkzlnzruj.mp4: need at least one array to concatenate\nPrediction error on video qdqdsaiitt.mp4: need at least one array to concatenate\nPrediction error on video qlqhjcshpk.mp4: need at least one array to concatenate\n\nPrediction error on video qhsehzgxqj.mp4: need at least one array to concatenate\nPrediction error on video qsjiypnjwi.mp4: need at least one array to concatenate\nPrediction error on video qlvsqdroqo.mp4: need at least one array to concatenate\nPrediction error on video qqnlrngaft.mp4: need at least one array to concatenatePrediction error on video qxyrtwozyw.mp4: need at least one array to concatenate\n\nPrediction error on video qyyhuvqmyf.mp4: need at least one array to concatenate\nPrediction error on video qooxnxqqjb.mp4: need at least one array to concatenate\nPrediction error on video rcecrgeotc.mp4: need at least one array to concatenate\nPrediction error on video rcjfxxhcal.mp4: need at least one array to concatenate\nPrediction error on video rfjuhbnlro.mp4: need at least one array to concatenate\nPrediction error on video rerpivllud.mp4: need at least one array to concatenate\nPrediction error on video qswlzfgcgj.mp4: need at least one array to concatenate\nPrediction error on video rklawjhbpv.mp4: need at least one array to concatenate\nPrediction error on video rfwxcinshk.mp4: need at least one array to concatenate\nPrediction error on video rmlzgerevr.mp4: need at least one array to concatenatePrediction error on video rktrpsdlci.mp4: need at least one array to concatenate\nPrediction error on video rnfcjxynfa.mp4: need at least one array to concatenate\nPrediction error on video rrrfjhugvb.mp4: need at least one array to concatenate\nPrediction error on video rmufsuogzn.mp4: need at least one array to concatenate\nPrediction error on video ruhtnngrqv.mp4: need at least one array to concatenate\nPrediction error on video rtpbawlmxr.mp4: need at least one array to concatenate\n\nPrediction error on video rvvpazsffd.mp4: need at least one array to concatenate\nPrediction error on video rxdoimqble.mp4: need at least one array to concatenate\nPrediction error on video rukyxomwcx.mp4: need at least one array to concatenate\nPrediction error on video scrbqgpvzz.mp4: need at least one array to concatenate\nPrediction error on video scbdenmaed.mp4: need at least one array to concatenate\nPrediction error on video sfsayjgzrh.mp4: need at least one array to concatenate\nPrediction error on video siebfpwuhu.mp4: need at least one array to concatenate\nPrediction error on video sjinmmbipg.mp4: need at least one array to concatenate\nPrediction error on video shnsajrsow.mp4: need at least one array to concatenatePrediction error on video sjkfxrlxxs.mp4: need at least one array to concatenate\n\nPrediction error on video sktpeppbkc.mp4: need at least one array to concatenate\nPrediction error on video ryxaqpfubf.mp4: need at least one array to concatenate\nPrediction error on video sngjsueuhs.mp4: need at least one array to concatenate\nPrediction error on video sjwywglgym.mp4: need at least one array to concatenate\nPrediction error on video sodvtfqbpf.mp4: need at least one array to concatenate\nPrediction error on video sqixhnilfm.mp4: need at least one array to concatenate\nPrediction error on video snlyjbnpgw.mp4: need at least one array to concatenate\nPrediction error on video sufvvwmbha.mp4: need at least one array to concatenate\nPrediction error on video sylnrepacf.mp4: need at least one array to concatenate\nPrediction error on video swsaoktwgi.mp4: need at least one array to concatenate\nPrediction error on video syuxttuyhm.mp4: need at least one array to concatenate\nPrediction error on video sznkemeqro.mp4: need at least one array to concatenate\nPrediction error on video tejfudfgpq.mp4: need at least one array to concatenate\nPrediction error on video syxobtuucp.mp4: need at least one array to concatenate\nPrediction error on video srfefmyjvt.mp4: need at least one array to concatenatePrediction error on video temjefwaas.mp4: need at least one array to concatenate\nPrediction error on video tgawasvbbr.mp4: need at least one array to concatenate\nPrediction error on video tjuihawuqm.mp4: need at least one array to concatenate\nPrediction error on video tjywwgftmv.mp4: need at least one array to concatenate\n\nPrediction error on video toinozytsp.mp4: need at least one array to concatenate\nPrediction error on video temeqbmzxu.mp4: need at least one array to concatenate\nPrediction error on video tvhjcfnqtg.mp4: need at least one array to concatenate\nPrediction error on video tyjpjpglgx.mp4: need at least one array to concatenate\nPrediction error on video txmnoyiyte.mp4: need at least one array to concatenatePrediction error on video tynfsthodx.mp4: need at least one array to concatenate\nPrediction error on video ucthmsajay.mp4: need at least one array to concatenate\nPrediction error on video txnmkabufs.mp4: need at least one array to concatenate\nPrediction error on video udxqbhgvvx.mp4: need at least one array to concatenate\nPrediction error on video uhakqelqri.mp4: need at least one array to concatenate\n\nPrediction error on video uhrqlmlclw.mp4: need at least one array to concatenate\nPrediction error on video upmgtackuf.mp4: need at least one array to concatenate\nPrediction error on video uoccaiathd.mp4: need at least one array to concatenatePrediction error on video uubgqnvfdl.mp4: need at least one array to concatenate\n\nPrediction error on video usqqvxcjmg.mp4: need at least one array to concatenate\nPrediction error on video uxuvkrjhws.mp4: need at least one array to concatenatePrediction error on video uvrzaczrbx.mp4: need at least one array to concatenate\nPrediction error on video vajkicalux.mp4: need at least one array to concatenate\n\nPrediction error on video vbcgoyxsvn.mp4: need at least one array to concatenate\nPrediction error on video vdtsbqidjb.mp4: need at least one array to concatenate\nPrediction error on video uqvxjfpwdo.mp4: need at least one array to concatenate\nPrediction error on video vhbbwdflyh.mp4: need at least one array to concatenatePrediction error on video vjljdfopjg.mp4: need at least one array to concatenate\n\nPrediction error on video viteugozpv.mp4: need at least one array to concatenate\nPrediction error on video vizerpsvbz.mp4: need at least one array to concatenate\nPrediction error on video vmxfwxgdei.mp4: need at least one array to concatenate\nPrediction error on video voawxrmqyl.mp4: need at least one array to concatenate\nPrediction error on video vokrpfjpeb.mp4: need at least one array to concatenate\nPrediction error on video vssmlqoiti.mp4: need at least one array to concatenate\nPrediction error on video vurjckblge.mp4: need at least one array to concatenatePrediction error on video vvfszaosiv.mp4: need at least one array to concatenatePrediction error on video vtunvalyji.mp4: need at least one array to concatenate\n\n\nPrediction error on video vnlzxqwthl.mp4: need at least one array to concatenate\nPrediction error on video vwxednhlwz.mp4: need at least one array to concatenate\nPrediction error on video wadvzjhwtw.mp4: need at least one array to concatenate\nPrediction error on video wclvkepakb.mp4: need at least one array to concatenate\nPrediction error on video wcqvzujamg.mp4: need at least one array to concatenate\nPrediction error on video waucvvmtkq.mp4: need at least one array to concatenate\nPrediction error on video wcssbghcpc.mp4: need at least one array to concatenate\nPrediction error on video wcvsqnplsk.mp4: need at least one array to concatenate\nPrediction error on video wixbuuzygv.mp4: need at least one array to concatenate\nPrediction error on video wjhpisoeaj.mp4: need at least one array to concatenatePrediction error on video wndursivcx.mp4: need at least one array to concatenatePrediction error on video wmoqzxddkb.mp4: need at least one array to concatenate\n\nPrediction error on video wnlubukrki.mp4: need at least one array to concatenate\nPrediction error on video wfzjxzhdkj.mp4: need at least one array to concatenate\n\nPrediction error on video wqysrieiqu.mp4: need at least one array to concatenate\nPrediction error on video wvgviwnwob.mp4: need at least one array to concatenate\nPrediction error on video xcruhaccxc.mp4: need at least one array to concatenate\nPrediction error on video wynotylpnm.mp4: need at least one array to concatenatePrediction error on video xitgdpzbxv.mp4: need at least one array to concatenate\n\nPrediction error on video xhtppuyqdr.mp4: need at least one array to concatenate\nPrediction error on video xljemofssi.mp4: need at least one array to concatenate\nPrediction error on video xmkwsnuzyq.mp4: need at least one array to concatenate\nPrediction error on video xphdfgmfmz.mp4: need at least one array to concatenate\nPrediction error on video xrtvqhdibb.mp4: need at least one array to concatenate\nPrediction error on video xugmhbetrw.mp4: need at least one array to concatenate\nPrediction error on video xjvxtuakyd.mp4: need at least one array to concatenate\nPrediction error on video xxzefxwyku.mp4: need at least one array to concatenate\nPrediction error on video xdezcezszc.mp4: need at least one array to concatenatePrediction error on video yaxgpxhavq.mp4: need at least one array to concatenate\nPrediction error on video ybbrkacebd.mp4: need at least one array to concatenate\n\nPrediction error on video yhjlnisfel.mp4: need at least one array to concatenate\nPrediction error on video yietrwuncf.mp4: need at least one array to concatenate\nPrediction error on video yarpxfqejd.mp4: need at least one array to concatenate\nPrediction error on video yhylappzid.mp4: need at least one array to concatenate\nPrediction error on video yllztsrwjw.mp4: need at least one array to concatenate\nPrediction error on video yiykshcbaz.mp4: need at least one array to concatenate\nPrediction error on video ylxwcwhjjd.mp4: need at least one array to concatenate\nPrediction error on video yljecirelf.mp4: need at least one array to concatenate\nPrediction error on video yronlutbgm.mp4: need at least one array to concatenate\nPrediction error on video ypbtpunjvm.mp4: need at least one array to concatenate\nPrediction error on video yqhouqakbx.mp4: need at least one array to concatenate\nPrediction error on video ystdtnetgj.mp4: need at least one array to concatenate\nPrediction error on video ytddugrwph.mp4: need at least one array to concatenate\nPrediction error on video yoyhmxtrys.mp4: need at least one array to concatenate\nPrediction error on video ywauoonmlr.mp4: need at least one array to concatenate\nPrediction error on video ytopzxrswu.mp4: need at least one array to concatenate\nPrediction error on video yxadevzohx.mp4: need at least one array to concatenate\nPrediction error on video yxirnfyijn.mp4: need at least one array to concatenate\nPrediction error on video yxvmusxvcz.mp4: need at least one array to concatenate\nPrediction error on video yzuestxcbq.mp4: need at least one array to concatenate\nPrediction error on video ywxpquomgt.mp4: need at least one array to concatenate\nPrediction error on video zcxcmneefk.mp4: need at least one array to concatenate\nPrediction error on video zfobicuigx.mp4: need at least one array to concatenate\nPrediction error on video zfrrixsimm.mp4: need at least one array to concatenate\nPrediction error on video zgbhzkditd.mp4: need at least one array to concatenate\nPrediction error on video zbgssotnjm.mp4: need at least one array to concatenate\nPrediction error on video ziipxxchai.mp4: need at least one array to concatenate\nPrediction error on video zgjosltkie.mp4: need at least one array to concatenate\nPrediction error on video zmxeiipnqb.mp4: need at least one array to concatenatePrediction error on video ztyvglkcsf.mp4: need at least one array to concatenate\n\nPrediction error on video ztyuiqrhdk.mp4: need at least one array to concatenate\nPrediction error on video zuwwbbusgl.mp4: need at least one array to concatenatePrediction error on video zyufpqvpyu.mp4: need at least one array to concatenate\nPrediction error on video zxacihctqp.mp4: need at least one array to concatenate\n\nPrediction error on video zzmgnglanj.mp4: need at least one array to concatenate\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df_resnext = pd.DataFrame({\"filename\": test_videos, \"label\": predictions})\n",
        "submission_df_resnext.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "y1EHO9KJmBdB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "51FUcrCimBdB"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}